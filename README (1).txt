
README.txt

Project Title:
Energy Consumption Prediction using Apache Spark on AWS

---

1. Cloud Environment Setup (AWS)

EC2 + Spark + HDFS Cluster

1. Launch EC2 Instances:
   - Use Ubuntu 20.04 AMIs.
   - Assign public IP, allow inbound ports: 22, 8080, 7077, 4040.

2. SSH into Instances:
   ssh -i "your-key.pem" ubuntu@your-ec2-public-ip

3. Install Java, Scala, Spark, Python 3, and Hadoop:
   sudo apt update && sudo apt install openjdk-11-jdk scala python3-pip -y
   wget https://downloads.apache.org/spark/spark-3.3.2/spark-3.3.2-bin-hadoop3.tgz
   tar xvf spark-3.3.2-bin-hadoop3.tgz
   mv spark-* spark

4. Set Environment Variables:
   Add to ~/.bashrc:
   export SPARK_HOME=~/spark
   export PATH=$SPARK_HOME/bin:$PATH
   export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64

5. Install and Configure HDFS (Hadoop):
   - Install Hadoop on all nodes
   - Configure core-site.xml and hdfs-site.xml for single-node or multi-node
   - Format Namenode:
     hdfs namenode -format
   - Start HDFS:
     start-dfs.sh
   - Verify with:
     hdfs dfs -ls /

6. Configure Spark Master/Worker:
   - On master node:
     $SPARK_HOME/sbin/start-master.sh
   - On worker node:
     $SPARK_HOME/sbin/start-slave.sh spark://<master-public-ip>:7077

---

2. Model Training with Spark

1. Upload Training Data to HDFS:
   hdfs dfs -put TrainingDataset.csv /user/ubuntu/

2. Run Training Script:
   spark-submit --master spark://<master-ip>:7077 train_model.py

3. Output:
   - Trained model saved to HDFS or local FS as specified
   - RMSE, MAE, RÂ² printed in logs

---

3. Application Prediction

A. Without Docker

1. Install Dependencies Locally:
   pip install pyspark pandas flask joblib

2. Run App:
   python3 app.py

3. Access App:
   http://localhost:5000

B. With Docker

1. Build Docker Image:
   docker build -t energy-predictor .

2. Run Docker Container:
   docker run -p 5000:5000 energy-predictor

3. Access App:
   http://localhost:5000

---

4. Code Attribution

A. Code Generated by ChatGPT/Copilots
- train_model.py structure
- Dockerfile
- Spark ML pipeline base

B. Code Written by Me
- Spark ML integration, feature engineering, normalization, evaluation
- Full HDFS integration and troubleshooting
- app.py prediction logic, Docker testing

C. Code Adapted from ChatGPT
- Preprocessing pipeline (MinMaxScaler, ChisqSelector)
- Cluster mode config tuning
- Dockerfile dependency fixes

---

5. My Experience Using ChatGPT/Copilots

ChatGPT was very helpful in setting up Spark ML quickly. It saved time on boilerplate and config, while I handled all real-world testing and debugging. HDFS integration was particularly tricky, and I had to rely more on documentation and trial than AI suggestions. Still, it made me more confident and productive in completing the project.

---
